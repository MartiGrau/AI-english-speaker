{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f699813-3469-4781-a69d-453f9e8c1a64",
   "metadata": {},
   "source": [
    "> **Date:** 03/09/23\n",
    "# Speach to text model\n",
    "\n",
    "#### Goal:\n",
    "Find a model for speach to text generation.  \n",
    "\n",
    "\n",
    "#### Resources:\n",
    "- Creating YouTube Captions with Wav2Vec [Link Colab](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)\n",
    "- Whisper Large V3 [LINK](https://huggingface.co/openai/whisper-large-v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407789d6-1211-47e2-a1de-8f673d8b2df7",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d9a14-2eae-4840-8bc3-8f827d7d99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers moviepy torch librosa accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc3947-e2e5-4c8e-8e79-80401e26563f",
   "metadata": {},
   "source": [
    "## Testing Whisper Large V3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df644efb-95c5-41a6-8453-bb003212a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from IPython.display import Audio\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b89987-92f4-4faf-a4f7-0b347c172d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your mp4 or from public audio\n",
    "my_audio = \"marti_test.mp4\"\n",
    "\n",
    "if not Path(my_audio).exists():\n",
    "    print(\"Your file does not exists. We will load a public audio file: librispeech_long\")\n",
    "    public_file = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "    my_audio = public_file[0][\"audio\"]\n",
    "    result = pipe(my_audio)\n",
    "    print(result[\"text\"]), display(Audio(dataset['audio'][0]['array'], rate=dataset['audio'][0]['sampling_rate']))\n",
    "\n",
    "else:\n",
    "    result = pipe(my_audio)\n",
    "    print(result[\"text\"]), display(Audio(my_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536f83e-4c32-4719-b411-fd987a67071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your mp4 or from public audio\n",
    "my_audio = \"marti_test.mp4\"\n",
    "result = pipe(my_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc65ad-4e30-4b92-933f-f0e01bf4d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db1557-399d-49ed-8241-1bb2356e54af",
   "metadata": {},
   "source": [
    "## Testing Wav2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c699f4d-84b7-4342-a2bb-b8fd737de644",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f8704-dd03-4849-b1d5-2da03555e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC\n",
    "import moviepy.editor as mp\n",
    "import torch\n",
    "import librosa\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2d521-cd2c-4173-8cf1-becbf064a960",
   "metadata": {},
   "source": [
    "#### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5351b4dd-d0d9-4e7c-8522-f4bb2e730293",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936fc3b-82c5-4a99-8fab-67f8e7347e19",
   "metadata": {},
   "source": [
    "#### Extract Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416b81e-99f4-4824-a50d-8f20062fb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = mp.VideoFileClip(\"marti_test.mp4\")\n",
    "end = min(clip.duration, end)\n",
    "\n",
    "# Save the paths for later\n",
    "clip_paths = []\n",
    "\n",
    "# Extract Audio-only from mp4\n",
    "for i in range(start, int(end), 10):\n",
    "  sub_end = min(i+10, end)\n",
    "  sub_clip = clip.subclip(i,sub_end)\n",
    "\n",
    "  sub_clip.audio.write_audiofile(\"audio_\" + str(i) + \".mp3\")\n",
    "  clip_paths.append(\"audio_\" + str(i) + \".mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f50a1b-3a88-4c82-b289-df5dca5e4f7e",
   "metadata": {},
   "source": [
    "#### Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0632b21-cb88-487a-933e-2d178854bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = \"\"\n",
    "\n",
    "for path in clip_paths:\n",
    "    # Load the audio with the librosa library\n",
    "    input_audio, _ = librosa.load(path, \n",
    "                                sr=16000)\n",
    "\n",
    "    # Tokenize the audio\n",
    "    input_values = tokenizer(input_audio, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "\n",
    "    # Feed it through Wav2Vec & choose the most probable tokens\n",
    "    with torch.no_grad():\n",
    "      logits = model(input_values).logits\n",
    "      predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Decode & add to our caption string\n",
    "    transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "    cc += transcription + \" \"\n",
    "\n",
    "# Here's your caption!\n",
    "# Note that there may be mistakes especially if the audio is noisy or there are uncommon words\n",
    "# If you picked the default video and change start to 0, you will see that the model gets confused by the word \"Anakin\"\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006062ee-66db-4055-9f11-35d66bc63a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
