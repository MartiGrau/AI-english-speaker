{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f699813-3469-4781-a69d-453f9e8c1a64",
   "metadata": {},
   "source": [
    "> **Date:** 03/09/23\n",
    "# Speach to text model\n",
    "\n",
    "#### Goal:\n",
    "Find a model for speach to text generation.  \n",
    "\n",
    "\n",
    "#### Resources:\n",
    "- Creating YouTube Captions with Wav2Vec [Link Colab](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)\n",
    "- Whisper Large V3 [LINK](https://huggingface.co/openai/whisper-large-v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407789d6-1211-47e2-a1de-8f673d8b2df7",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d9a14-2eae-4840-8bc3-8f827d7d99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers moviepy torch librosa accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc3947-e2e5-4c8e-8e79-80401e26563f",
   "metadata": {},
   "source": [
    "## Testing Whisper Large V3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df644efb-95c5-41a6-8453-bb003212a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from IPython.display import Audio\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b89987-92f4-4faf-a4f7-0b347c172d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your mp4 or from public audio\n",
    "my_audio = \"marti_test.mp4\"\n",
    "\n",
    "if not Path(my_audio).exists():\n",
    "    print(\"Your file does not exists. We will load a public audio file: librispeech_long\")\n",
    "    public_file = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "    my_audio = public_file[0][\"audio\"]\n",
    "    result = pipe(my_audio)\n",
    "    print(result[\"text\"]), display(Audio(dataset['audio'][0]['array'], rate=dataset['audio'][0]['sampling_rate']))\n",
    "\n",
    "else:\n",
    "    result = pipe(my_audio)\n",
    "    print(result[\"text\"]), display(Audio(my_audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db1557-399d-49ed-8241-1bb2356e54af",
   "metadata": {},
   "source": [
    "## Testing Wav2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c699f4d-84b7-4342-a2bb-b8fd737de644",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4f8704-dd03-4849-b1d5-2da03555e40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgrau/personal/environments/ai-speaker/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC\n",
    "import moviepy.editor as mp\n",
    "import torch\n",
    "import librosa\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2d521-cd2c-4173-8cf1-becbf064a960",
   "metadata": {},
   "source": [
    "#### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5351b4dd-d0d9-4e7c-8522-f4bb2e730293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936fc3b-82c5-4a99-8fab-67f8e7347e19",
   "metadata": {},
   "source": [
    "#### Extract Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b416b81e-99f4-4824-a50d-8f20062fb353",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'video_fps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clip \u001b[38;5;241m=\u001b[39m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoFileClip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmarti_test.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(clip\u001b[38;5;241m.\u001b[39mduration, end)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save the paths for later\u001b[39;00m\n",
      "File \u001b[0;32m~/personal/environments/ai-speaker/lib/python3.10/site-packages/moviepy/video/io/VideoFileClip.py:88\u001b[0m, in \u001b[0;36mVideoFileClip.__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Make a reader\u001b[39;00m\n\u001b[1;32m     87\u001b[0m pix_fmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgba\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_mask \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb24\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader \u001b[38;5;241m=\u001b[39m \u001b[43mFFMPEG_VideoReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpix_fmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpix_fmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtarget_resolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_resolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mresize_algo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize_algorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mfps_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Make some of the reader's attributes accessible from the clip\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader\u001b[38;5;241m.\u001b[39mduration\n",
      "File \u001b[0;32m~/personal/environments/ai-speaker/lib/python3.10/site-packages/moviepy/video/io/ffmpeg_reader.py:37\u001b[0m, in \u001b[0;36mFFMPEG_VideoReader.__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     35\u001b[0m infos \u001b[38;5;241m=\u001b[39m ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[1;32m     36\u001b[0m                            fps_source)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfps \u001b[38;5;241m=\u001b[39m \u001b[43minfos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo_fps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m infos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation \u001b[38;5;241m=\u001b[39m infos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_rotation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'video_fps'"
     ]
    }
   ],
   "source": [
    "clip = mp.VideoFileClip(\"marti_test.mp4\")\n",
    "end = min(clip.duration, end)\n",
    "\n",
    "# Save the paths for later\n",
    "clip_paths = []\n",
    "\n",
    "# Extract Audio-only from mp4\n",
    "for i in range(start, int(end), 10):\n",
    "  sub_end = min(i+10, end)\n",
    "  sub_clip = clip.subclip(i,sub_end)\n",
    "\n",
    "  sub_clip.audio.write_audiofile(\"audio_\" + str(i) + \".mp3\")\n",
    "  clip_paths.append(\"audio_\" + str(i) + \".mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f50a1b-3a88-4c82-b289-df5dca5e4f7e",
   "metadata": {},
   "source": [
    "#### Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0632b21-cb88-487a-933e-2d178854bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = \"\"\n",
    "\n",
    "for path in clip_paths:\n",
    "    # Load the audio with the librosa library\n",
    "    input_audio, _ = librosa.load(path, \n",
    "                                sr=16000)\n",
    "\n",
    "    # Tokenize the audio\n",
    "    input_values = tokenizer(input_audio, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "\n",
    "    # Feed it through Wav2Vec & choose the most probable tokens\n",
    "    with torch.no_grad():\n",
    "      logits = model(input_values).logits\n",
    "      predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Decode & add to our caption string\n",
    "    transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "    cc += transcription + \" \"\n",
    "\n",
    "# Here's your caption!\n",
    "# Note that there may be mistakes especially if the audio is noisy or there are uncommon words\n",
    "# If you picked the default video and change start to 0, you will see that the model gets confused by the word \"Anakin\"\n",
    "print(cc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
